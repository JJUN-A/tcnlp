{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrain_bert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMy/1hbH3W/bKrNYsoBpIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhouNLP/tcnlp/blob/master/bert_model/pretrain_bert/pretrain_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5vxkNRVwZHg",
        "colab_type": "text"
      },
      "source": [
        "预训练bert_base，这里只用了训练集和测试集A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0yRNW0Sv6kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('../train_set.csv', sep='\\t', nrows=None)\n",
        "test_df = pd.read_csv('../test_a.csv', sep='\\t', nrows=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA5QmTBkwoZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 将语料处理成txt文件，之所以分成若干小文件，是为了避免创建tfrecord时爆内存，根据你内存的大小，可以调节划分的文件数量\n",
        "\n",
        "train_l = 0\n",
        "test_l = 0\n",
        "\n",
        "!mkdir data\n",
        "for i in range(0, 200000, 20000):\n",
        "    with open('./data/train_{}.txt'.format(i), 'a+') as f:\n",
        "        for doc in train_df['text'][i:i+20000]:        \n",
        "            f.write(doc+'\\n')\n",
        "            train_l += 1\n",
        "            f.write('\\n')\n",
        "print(train_l)\n",
        "for i in range(0, 50000, 25000):\n",
        "    with open('./data/test_{}.txt'.format(i), 'a+') as f:\n",
        "        for doc in test_df['text'][i:i+25000]:        \n",
        "            f.write(doc+'\\n')\n",
        "            test_l += 1\n",
        "            f.write('\\n')\n",
        "print(test_l)\n",
        "\n",
        "print(train_l+test_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee_1mDlD2xZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8VZgGa_x2XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 创建用于预训练的tfrecord文件\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_0.txt --output_file=./records/train_0.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_20000.txt --output_file=./records/train_1.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_40000.txt --output_file=./records/train_2.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_60000.txt --output_file=./records/train_3.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_80000.txt --output_file=./records/train_4.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_100000.txt --output_file=./records/train_5.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_120000.txt --output_file=./records/train_6.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_140000.txt --output_file=./records/train_7.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_160000.txt --output_file=./records/train_8.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/train_180000.txt --output_file=./records/train_9.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/test_0.txt --output_file=./records/train_10.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\n",
        "\n",
        "!python create_pretraining_data.py --input_file=./data/test_25000.txt --output_file=./records/train_11.tfrecord \\\n",
        "--vocab_file=bert_vocab.txt --max_seq_length=256 --max_predictions_per_seq=32 --do_lower_case=True \\\n",
        "--do_whole_word_mask=True \\--masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sntjDZZTytZ8",
        "colab_type": "text"
      },
      "source": [
        "run_pretraining.sh 是预训练脚本，最好在终端运行，更加稳定。16G显存训练bert_base模型batch_size可以设为16，设成32就会OOM。当然batch_size可以设为任意正整数，这里是习惯上取2的指数幂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Wg6mMo0mz8",
        "colab_type": "text"
      },
      "source": [
        "最后用convert_checkpoint.sh 将checkpiont转成pytorch模型"
      ]
    }
  ]
}