{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhouNLP/tcnlp/blob/master/lstm_model/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsE5ssE_Hn8W",
        "colab_type": "text"
      },
      "source": [
        "使用gensim训练word2vec词向量的代码，除了gensim参数以外，跟天池论坛的基本一致\n",
        "\n",
        "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.18.64065cbbRfhdqJ&postId=118268\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TezTrrIHd_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import random\n",
        "import time\n",
        "# import sys\n",
        "# sys.path.append('/home/aistudio/external-libraries')\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
        "\n",
        "# set seed \n",
        "seed = 666\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "# torch.cuda.manual_seed(seed)\n",
        "# torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMvkmQdUHd_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data to 10 fold\n",
        "fold_num = 10\n",
        "data_file = 'train_set.csv'\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def all_data2fold(fold_num, num=200000):\n",
        "    fold_data = []\n",
        "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
        "    texts = f['text'].tolist()[:num]\n",
        "    labels = f['label'].tolist()[:num]\n",
        "\n",
        "    total = len(labels)\n",
        "\n",
        "    index = list(range(total))\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    all_texts = []\n",
        "    all_labels = []\n",
        "    for i in index:\n",
        "        all_texts.append(texts[i])\n",
        "        all_labels.append(labels[i])\n",
        "\n",
        "    label2id = {}\n",
        "    for i in range(total):\n",
        "        label = str(all_labels[i])\n",
        "        if label not in label2id:\n",
        "            label2id[label] = [i]\n",
        "        else:\n",
        "            label2id[label].append(i)\n",
        "\n",
        "    all_index = [[] for _ in range(fold_num)]\n",
        "    for label, data in label2id.items():\n",
        "        # print(label, len(data))\n",
        "        batch_size = int(len(data) / fold_num)\n",
        "        other = len(data) - batch_size * fold_num\n",
        "        for i in range(fold_num):\n",
        "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
        "            # print(cur_batch_size)\n",
        "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
        "            all_index[i].extend(batch_data)\n",
        "\n",
        "    batch_size = int(total / fold_num)\n",
        "    other_texts = []\n",
        "    other_labels = []\n",
        "    other_num = 0\n",
        "    start = 0\n",
        "    for fold in range(fold_num):\n",
        "        num = len(all_index[fold])\n",
        "        texts = [all_texts[i] for i in all_index[fold]]\n",
        "        labels = [all_labels[i] for i in all_index[fold]]\n",
        "\n",
        "        if num > batch_size:\n",
        "            fold_texts = texts[:batch_size]\n",
        "            other_texts.extend(texts[batch_size:])\n",
        "            fold_labels = labels[:batch_size]\n",
        "            other_labels.extend(labels[batch_size:])\n",
        "            other_num += num - batch_size\n",
        "        elif num < batch_size:\n",
        "            end = start + batch_size - num\n",
        "            fold_texts = texts + other_texts[start: end]\n",
        "            fold_labels = labels + other_labels[start: end]\n",
        "            start = end\n",
        "        else:\n",
        "            fold_texts = texts\n",
        "            fold_labels = labels\n",
        "\n",
        "        assert batch_size == len(fold_labels)\n",
        "\n",
        "        # shuffle\n",
        "        index = list(range(batch_size))\n",
        "        np.random.shuffle(index)\n",
        "\n",
        "        shuffle_fold_texts = []\n",
        "        shuffle_fold_labels = []\n",
        "        for i in index:\n",
        "            shuffle_fold_texts.append(fold_texts[i])\n",
        "            shuffle_fold_labels.append(fold_labels[i])\n",
        "\n",
        "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
        "        fold_data.append(data)\n",
        "\n",
        "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
        "\n",
        "    return fold_data\n",
        "\n",
        "\n",
        "fold_data = all_data2fold(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61dfFhWRHd_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build train data for word2vec\n",
        "fold_id = 10\n",
        "\n",
        "train_texts = []\n",
        "for i in range(0, fold_id):\n",
        "    data = fold_data[i]\n",
        "    train_texts.extend(data['text'])\n",
        "    \n",
        "logging.info('Total %d docs.' % len(train_texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT6tq83pHd_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pd = pd.read_csv('test_a.csv', sep='\\t', encoding='UTF-8') # 加入了测试集A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT96bmxYHd_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in test_pd['text']:\r\n",
        "    train_texts.append(i)\r\n",
        "logging.info('Total %d docs.' % len(train_texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxYFpHwlHd_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 为了看到训练时的loss，加入了回调函数，但是似乎有BUG，训练多了loss会变为0\n",
        "\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "class callback(CallbackAny2Vec):\n",
        "    '''Callback to print loss after each epoch.'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 1\n",
        "        self.loss_to_be_subed = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        loss_now = loss - self.loss_to_be_subed\n",
        "        self.loss_to_be_subed = loss\n",
        "        with open('w2vlog.txt', 'a+') as f:\n",
        "            f.write('Loss after epoch {}: {}'.format(self.epoch, loss_now)+'\\n')\n",
        "        print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),'Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
        "        self.epoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLFqrlZvHd_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info('Start training...')\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.WARNING)\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "num_features = 200     # Word vector dimensionality\n",
        "num_workers = 12       # Number of threads to run in parallel\n",
        "\n",
        "train_texts_ = list(map(lambda x: list(x.split()), train_texts))\n",
        "\n",
        "# 采用skip-gram，负采样，训练10轮，12核CPU大约需要4个小时\n",
        "model = Word2Vec(train_texts_, sg=1, workers=num_workers, size=num_features, compute_loss=True,\n",
        "                 callbacks=[callback()], iter=10, hs=0, window=10)\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# save model\n",
        "model.save(\"./word2vec.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_sRB_8uHd_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model\n",
        "model = Word2Vec.load(\"./word2vec.bin\")\n",
        "\n",
        "# convert format\n",
        "model.wv.save_word2vec_format('./word2vec.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}