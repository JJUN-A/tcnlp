{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6h/iInFWZsoRzvh5mfCnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhouNLP/tcnlp/blob/master/lstm_model/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7sX3bP4HwM",
        "colab_type": "text"
      },
      "source": [
        "此模型根据其他选手公开的模型修改而来，原模型https://github.com/LogicJake/tianchi_nlp/blob/master/model_lstm_5fold.ipynb\n",
        "\n",
        "原模型测试集A f1 95.63，调整后的模型测试集A 96.68，测试集B 96.74\n",
        "\n",
        "如果在colab里面运行，需自行上传除.ipynb以外的其他项目文件\n",
        "\n",
        "Keras==2.3.1\n",
        "\n",
        "tensorflow==1.15.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJWzbZ2n7tX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/train_set.csv.zip\n",
        "!wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/test_a.csv.zip\n",
        "!wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/test_b.csv.zip\n",
        "!unzip train_set.csv.zip\n",
        "!unzip test_a.csv.zip\n",
        "!unzip test_b.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzbsEY9wERrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://drive.deepnlp.workers.dev/test_a_fake_label.csv  # 伪标签文件"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxRSLpqd6fr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "187abioW3Q7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "# from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import Model\n",
        "from keras.layers import Conv1D, Embedding, Input, Bidirectional, CuDNNLSTM, Dense, Concatenate, Masking, LSTM, SpatialDropout1D\n",
        "from keras.layers import BatchNormalization, Dropout, Activation\n",
        "from keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, GlobalAvgPool1D, GlobalMaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "from keras.utils import to_categorical\n",
        "from keras_radam import RAdam\n",
        "from keras_lookahead import Lookahead\n",
        "# from keras import regularizers\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFiZQYdm37oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-rectified-adam\n",
        "!pip install keras-lookahead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYhbyayn387U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_seed(seed):\n",
        "#     random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.set_random_seed(seed)\n",
        "\n",
        "seed = 2020\n",
        "fix_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST2ZjmEF67JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('train_set.csv', sep='\\t')\n",
        "df_test = pd.read_csv('test_a.csv', sep='\\t')\n",
        "df_test_b = pd.read_csv('test_b.csv', sep='\\t')\n",
        "fake_label_a = pd.read_csv('test_a_fake_label.csv', sep='\\t')  #伪标签文件，如果从头训练，需要去掉此处及下面的相关代码\n",
        "df_data = df_train.append(df_test)\n",
        "df_data = df_data.reset_index(drop=True)\n",
        "df_data = df_data.append(df_test_b)\n",
        "df_data = df_data.reset_index(drop=True)\n",
        "print(df_data.shape, fake_label_a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43G90AuB7ZSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words_num = None\n",
        "seq_len = 2000\n",
        "embedding_dim = 200\n",
        "col = 'text'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI151akv8biP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Generate seqs')\n",
        "os.makedirs('seqs', exist_ok=True)\n",
        "seq_path = 'seqs/seqs_{}_{}.npy'.format(max_words_num, seq_len)\n",
        "word_index_path = 'seqs/word_index_{}_{}.npy'.format(max_words_num, seq_len)\n",
        "fake_path = 'seqs/fake_label.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQcAkEQa82qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 前后截取2000个字符，虽然截取3000会更好一点，但是训练时间会大大增加\n",
        "\n",
        "if not os.path.exists(seq_path) or not os.path.exists(word_index_path):\n",
        "    tokenizer = text.Tokenizer(num_words=max_words_num, lower=False, filters='')\n",
        "    tokenizer.fit_on_texts(df_data[col].values.tolist())\n",
        "    ids_doc = tokenizer.texts_to_sequences(df_data[col].values.tolist())\n",
        "    pre_post = [doc if len(doc) <= 2000 else doc[:1000]+doc[-1000:] for doc in ids_doc]                                   \n",
        "    seqs = sequence.pad_sequences(pre_post, maxlen=seq_len,\n",
        "                        padding='post', truncating='pre')\n",
        "    word_index = tokenizer.word_index\n",
        "#   下面是对伪标签的处理，全新训练可注释掉   \n",
        "    ids_doc_ = tokenizer.texts_to_sequences(fake_label_a[col].values.tolist())\n",
        "    pre_post_ = [doc if len(doc) <= 2000 else doc[:1000]+doc[-1000:] for doc in ids_doc_]                                   \n",
        "    fake_seqs = sequence.pad_sequences(pre_post_, maxlen=seq_len,\n",
        "                        padding='post', truncating='pre')\n",
        "    \n",
        "    np.save(fake_path, fake_seqs)\n",
        "    np.save(seq_path, seqs)\n",
        "    np.save(word_index_path, word_index)\n",
        "\n",
        "else:\n",
        "    fake_seqs = np.load(fake_path)\n",
        "    seqs = np.load(seq_path)\n",
        "    word_index = np.load(word_index_path, allow_pickle=True).item()\n",
        "    \n",
        "embedding_path = 'word2vec.txt'   # 提前训练好的200维word2vec词向量\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(embedding_path)\n",
        "\n",
        "embedding = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = model[word] if word in model else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding[i] = embedding_vector\n",
        "embedding = embedding / np.std(embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538z4AqQ-Nz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"# 模型训练\"\"\"\n",
        "\n",
        "os.makedirs('model', exist_ok=True)\n",
        "os.makedirs('sub', exist_ok=True)\n",
        "os.makedirs('prob', exist_ok=True)\n",
        "\n",
        "all_index = df_data[df_data['label'].notnull()].index.tolist()\n",
        "test_index = df_data[df_data['label'].isnull()].index.tolist()[-50000:]  # 因为同时包含了测试集A、B，所以预测的是最后50000个测试集B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX7eUuKt-1cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 这里模型的各层都加了name，是因为本来后面要用到load_weights(model_path, by_name=True)\n",
        "\n",
        "def build_model(emb, seq_len):\n",
        "    emb_layer = Embedding(\n",
        "        input_dim=emb.shape[0],\n",
        "        output_dim=emb.shape[1],\n",
        "        weights=[emb],\n",
        "        input_length=seq_len,\n",
        "        name='emb_word2vec',\n",
        "        trainable=False  # 这里我虽然设成了False，但是根据你词向量的具体情况，可能设成True会更好\n",
        "    )\n",
        "    \n",
        "    seq = Input(shape=(seq_len, ), name='seq_input')\n",
        "    seq_emb = emb_layer(seq)\n",
        "    \n",
        "    seq_emb = SpatialDropout1D(rate=0.2, name='drop_out1')(seq_emb)\n",
        "\n",
        "    lstm = Bidirectional(CuDNNLSTM(200, return_sequences=True, name='lstm'), name='bi_layer')(seq_emb)\n",
        "    lstm_avg_pool = GlobalAveragePooling1D(name='avg')(lstm)\n",
        "    lstm_max_pool = GlobalMaxPooling1D(name='max')(lstm)\n",
        "#     att = SeqWeightedAttention(name='wei_att')(lstm)\n",
        "    x = Concatenate(name='concat')([lstm_avg_pool,lstm_max_pool])\n",
        "#     x = att\n",
        "    \n",
        "    x = Dropout(0.2, name='drop_2')(Activation(activation='relu',name='acti')(BatchNormalization(name='bn')(Dense(1024,name='dense_1')(x))))\n",
        "    out = Dense(14, activation='softmax',name='dense_2')(x)\n",
        "    \n",
        "    model = Model(inputs=seq, outputs=out)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Lookahead(RAdam()), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmcn-_KbAO2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Evaluator(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.best_val_f1 = 0.\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "\n",
        "    def evaluate(self):\n",
        "        y_true = self.y_val\n",
        "        y_pred = self.model.predict(self.x_val).argmax(axis=1)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "        return f1\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_f1 = self.evaluate()\n",
        "        if val_f1 > self.best_val_f1:\n",
        "            self.best_val_f1 = val_f1\n",
        "        logs['val_f1'] = val_f1\n",
        "        print(f'val_f1: {val_f1:.5f}, best_val_f1: {self.best_val_f1:.5f}')\n",
        "\n",
        "bs = 256\n",
        "monitor = 'val_f1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjiKoeXJAS08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 这里我对伪标签的处理是，只加入各折的训练集，并未加入验证集。至于这样做是不是更好，没来得及做对比实验\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "for fold_id, (train_index, val_index) in enumerate(kfold.split(all_index, df_data.iloc[all_index]['label'])):\n",
        "#     train_x = seqs[train_index]\n",
        "    train_x = np.vstack((seqs[train_index], fake_seqs))\n",
        "    val_x = seqs[val_index]\n",
        "\n",
        "    label = df_data['label'].values\n",
        "    fake_label = fake_label_a['label'].values\n",
        "#     train_y = label[train_index]\n",
        "    train_y = np.hstack((label[train_index], fake_label))\n",
        "    val_y = label[val_index]\n",
        "    \n",
        "    model_path = 'model/lstm_{}.h5'.format(fold_id)\n",
        "    checkpoint = ModelCheckpoint(model_path, monitor=monitor, verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
        "    earlystopping = EarlyStopping(monitor=monitor, patience=5, verbose=1, mode='max')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=2, mode='max', verbose=1)\n",
        "    \n",
        "    model = build_model(embedding, seq_len)\n",
        "#     model_path = 'model/lstm_{}.h5'.format(fold_id)\n",
        "    model.load_weights(model_path, by_name=True)\n",
        "    model.fit(train_x, train_y, batch_size=bs, epochs=30,\n",
        "              validation_data=(val_x, val_y),\n",
        "              callbacks=[Evaluator(validation_data=(val_x, val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpyVbOlEAfee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"# 模型预测\"\"\"\n",
        "\n",
        "oof_pred = np.zeros((len(all_index), 14))\n",
        "test_pred = np.zeros((len(test_index), 14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbaI1hXkBjk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "for fold_id, (train_index, val_index) in enumerate(kfold.split(all_index, df_data.iloc[all_index]['label'])):\n",
        "    model = build_model(embedding, seq_len)\n",
        "    model_path = 'model/lstm_{}.h5'.format(fold_id)\n",
        "    model.load_weights(model_path)\n",
        "    \n",
        "    val_x = seqs[val_index]\n",
        "    prob = model.predict(val_x, batch_size=bs, verbose=1)\n",
        "    oof_pred[val_index] = prob\n",
        "    \n",
        "    test_x = seqs[test_index]\n",
        "    prob = model.predict(test_x, batch_size=bs, verbose=1)\n",
        "    df = pd.DataFrame(prob)\n",
        "    df.to_csv('sub/lstm_{}.csv'.format(fold_id), index=False, sep=',', header=False)\n",
        "    test_pred += prob / 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkmjlGC4Bn9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_oof = df_data.loc[all_index][['label']]\n",
        "df_oof['predict'] = np.argmax(oof_pred, axis=1)\n",
        "f1score = f1_score(df_oof['label'], df_oof['predict'], average='macro')\n",
        "print(f1score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmYKkTGRBtot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('prob/sub_5fold_lstm_{}.npy'.format(f1score), test_pred)\n",
        "np.save('prob/oof_5fold_lstm_{}.npy'.format(f1score), oof_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JidLeIV_BxhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub = pd.DataFrame()\n",
        "sub['label'] = np.argmax(test_pred, axis=1)\n",
        "sub.to_csv('sub/lstm_{}.csv'.format(f1score), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}